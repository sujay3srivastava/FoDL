# -*- coding: utf-8 -*-
"""AutoEncoder

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158er_54n80P5lO_QiNvwFt4FZjTgdywF
"""

from operator import mod
import random
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import os
import numpy as np
import sys
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import pandas as pd
from sklearn.decomposition import PCA
torch.manual_seed(3)
torch.cuda.manual_seed_all(3)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(3)
random.seed(3)
os.environ['PYTHONHASHSEED'] = str(3)

"""
Hyper Parameters
"""
num_epochs = 100
batch_size = 4096
#sizes = [48, 128, 64, 32, 64, 128, 48]
#sizes = [48 , 36 , 24 , 16 ,24 , 36, 48]
# sizes = [48 , 32 , 16 , 8 , 16 , 32 , 48]
#sizes = [48 , 24 , 12 , 4 ,12 , 24, 48]
sizes = [48 , 16 , 8 , 2 ,8 , 16 , 48]
num_feat = sizes[len(sizes) // 2]
mid_pt = len(sizes) // 2 + 1
patience = 3
learning_rate = float(5e-3)
if torch.cuda.is_available():
  device = 'cuda'
else:
  device = 'cpu'
save = True

class Dataset(Dataset):
    def __init__(self, X, y):
        super(Dataset, self).__init__()
        self.X = X
        self.y = y

    def __getitem__(self, index):
        sample = {}
        sample["x"] = torch.tensor(self.X[index], dtype=torch.float32)
        sample["y"] = torch.tensor(self.y[index], dtype=torch.float32)
        return sample

    def __len__(self):
        return len(self.X)

def load_dataset():
    classes = ['ragno', 'scoiattolo', 'cane', 'gatto', 'farfalla']

    ds = []
    y = []

    for i, name in enumerate(classes):
        csv_file = os.path.join("D:/Semester_6/CS6910", "csv", name + ".csv")
        df = pd.read_csv(csv_file).to_numpy()[:, 1:49]
        ds.append(df)
        y.extend(len(df) * [i])

    X = np.concatenate(ds, axis=0)
    X = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))
    y = np.reshape(y, [-1, 1])

    return X.astype(np.float32), y.astype(np.float32)

def make_dataloaders(X, y, batch_size=16, test_split=0.1):
    # Split the data into train and validation sets
    X_train, X_val, Y_train, Y_val = train_test_split(X, y, test_size=test_split)

    # Create train and validation datasets
    train_ds = Dataset(X_train, Y_train)
    val_ds = Dataset(X_val, Y_val)

    # Create train and validation data loaders
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=False)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, pin_memory=False)

    return train_loader, val_loader

def get_AANET_model():
    layers = []
    for (inp, out) in zip(sizes[:-2], sizes[1:]):
        layers.append(nn.Linear(inp, out))
        layers.append(nn.Tanh())

    # Add the final output layer
    out = nn.Linear(in_features=sizes[-2], out_features=sizes[-1])
    layers.append(out)

    # Create the neural network model using Sequential
    net = nn.Sequential(*layers)

    return net.to(device)

def get_classifier_model(num_feat):
    # Define the sizes of input and output layers for the classifier model
    sizes = [num_feat, 8, 5]

    layers = []
    for (inp, out) in zip(sizes[:-2], sizes[1:]):
        layers.append(nn.Linear(inp, out))
        layers.append(nn.ReLU())

    # Add the final output layer
    out = nn.Linear(sizes[-2], sizes[-1])
    layers.append(out)

    # Create the classifier model using Sequential
    net = nn.Sequential(*layers)

    return net.to(device)
'''
The `get_AANET_model` function creates an AANET (Artificially Augmented Neural Network) model using the specified sizes of input and output layers. It uses the `nn.Sequential` container to stack the layers in sequence.

The `get_classifier_model` function creates a classifier model with a specified number of input features and hidden layer sizes. It also uses the `nn.Sequential` container to stack the layers in sequence.
'''

def train_AANN(resume=False, num_feat=num_feat, batch_size=16):
  X, y = load_dataset()
  train_loader, val_loader = make_dataloaders(X, y, batch_size=batch_size)
  model = get_AANET_model()
  criterion = nn.MSELoss(reduction='mean')
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  epoch_start = 0
  if resume:
    ckpts = sorted(os.listdir('output\q1\checkpoints'))
    latest_ckpt = os.path.join("output\q1\checkpoints", ckpts[-1])
    print(latest_ckpt)
    state_dict = torch.load(latest_ckpt)['model_state_dict']
    model.load_state_dict(state_dict)
    # epoch_start = state_dict['epoch']
  avg_losses = []
  avg_losses_val = []
  last_loss = np.Inf
  for epoch in range(epoch_start, num_epochs):
    total_step = len(train_loader)
    model.train()
    sum_loss = 0
    for i, sample in enumerate(train_loader):
    # Move tensors to the configured device
      x = sample['x'].to(device)
      pred_x = model(x)
      loss = criterion(x, pred_x)
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      sum_loss += loss.item()
    avg_loss = sum_loss / total_step
    print('Loss(Train) of the AANN network after {} epochs {}'.format(epoch,
    avg_loss))
    avg_losses.append(avg_loss)
    with torch.no_grad():
      model.eval()
      total_step = len(val_loader)
      sum_loss = 0
      for i, sample in enumerate(val_loader):
      # Move tensors to the configured device
        x = sample['x'].to(device)
        # Forward pass
        x_pred = model(x)
        loss = criterion(x, x_pred)
        sum_loss += loss.item()
    avg_loss_val = sum_loss / total_step
    print('Loss(Val) of the AANN network after {} epochs {}'.format(epoch,
    avg_loss_val))
    avg_losses_val.append(avg_loss_val)
    if epoch % 50 == 0:
      torch.save({
      'model_state_dict': model.state_dict(),
      'optimizer_state_dict': optimizer.state_dict(),
       }, f"D:/Semester_6/CS6910/output/q1/checkpoints/ckpt_{epoch}.pth")
    if avg_loss_val > last_loss:
      trigger_times += 1
      print('Trigger Times:', trigger_times)
      if trigger_times >= patience:
        print('Early stopping!')
        torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        },
        f"D:/Semester_6/CS6910/output/q1/checkpoints/ckpt_{epoch}.pth")
        break
    else:
      print('trigger times: 0')
      trigger_times = 0
    last_loss = avg_loss_val
  plt.plot(avg_losses_val)
  plt.plot(avg_losses)
  plt.legend(['validation_loss', 'training_loss'])
  plt.title("Loss curve for Auto-Assosiative Networks's training")
  plt.xlabel("Epochs")
  plt.show()
  return model

# Commented out IPython magic to ensure Python compatibility.
def train_classifier(resume=False, num_feat=num_feat, mode="AANET",
  batch_size=16):
  if mode == "AANET":
    features, y = load_AANET_features()
  elif mode == "PCA":
    features, y = load_PCA_features(num_feat)

  train_loader, val_loader = make_dataloaders(X=features, y=y,
  batch_size=batch_size)
  model = get_classifier_model(num_feat)
  criterion = nn.CrossEntropyLoss(reduction='mean')
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  epoch_start = 0
  # if resume :
  # ckpts = sorted(os.listdir('output\q1\checkpoints'))
  # latest_ckpt = os.path.join("output\q1\checkpoints" , ckpts[-1])
  # print(latest_ckpt)
  # state_dict = torch.load(latest_ckpt)['model_state_dict']
  # model.load_state_dict(state_dict)
  # # epoch_start = state_dict['epoch']
  avg_losses = []
  avg_losses_val = []
  avg_acc = []
  last_loss = np.Inf
  for epoch in range(epoch_start, num_epochs):
    total_step = len(train_loader)
    model.train()
    sum_loss = 0
    for i, sample in enumerate(train_loader):
    # Move tensors to the configured device
      x = sample['x'].to(device)
      y = sample['y'].to(device).long().squeeze(1)
      pred_x = model(x)
      loss = criterion(pred_x, y)
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      sum_loss += loss.item()
    avg_loss = sum_loss / total_step
    print('Loss(Train) of the network with {} after {} epochs
    {}'.format(mode,epoch, avg_loss))
    avg_losses.append(avg_loss)
    with torch.no_grad():
      model.eval()
      total_step = len(val_loader)
      sum_loss, total, correct = 0, 0, 0
      for i, sample in enumerate(val_loader):
        # Move tensors to the configured device
        x = sample['x'].to(device)
        y = sample['y'].to(device).long().squeeze(1)
        # Forward pass
        x_pred = model(x)
        loss = criterion(x_pred, y)
        sum_loss += loss.item()
        _, predicted = torch.max(x_pred.data, 1)
        total += y.size(0)
        correct += (predicted == y).sum().item()
    acc = 100 * correct / total
    print('Accuracy of the network with {} after {} epochs {}
#     %'.format(mode,epoch, acc))
    avg_acc.append(acc)
    avg_loss_val = sum_loss / total_step
    print('Loss(Val) of the network with {} after {} epochs
    {}'.format(mode,epoch, avg_loss_val))
    avg_losses_val.append(avg_loss_val)
    if epoch % 200 == 0:
      torch.save({
      'model_state_dict': model.state_dict(),
      'optimizer_state_dict': optimizer.state_dict(),
      },
      f"D:/Semester_6/CS6910/output/q1/checkpoints/ckpt_classification_{epoch}.pth")
    if avg_loss_val > last_loss:
      trigger_times += 1
      print('Trigger Times:', trigger_times)
      if trigger_times >= patience:
        print('Early stopping!')
        torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        },
        f"D:/Semester_6/CS6910/output/q1/checkpoints_classification/ckpt_{epoch}.pth")
        break
    else:
      print('trigger times: 0')
      trigger_times = 0
    last_loss = avg_loss_val
  plt.plot(avg_losses_val)
  plt.plot(avg_losses)
  plt.legend(['validation_loss', 'training_loss'])
  plt.title(f"Loss curve for Classifier's training for {mode}")
  plt.show()
  plt.plot(avg_acc)
  plt.legend('Accuracy')
  plt.title(f"Accuracy for Classifier's training for {mode}")
  plt.ylabel('Accuracy')
  plt.xlabel('Epochs')
  plt.show()
  return model

def load_AANET_features():
  model = get_AANET_model()
  print(model)
  X, y = load_dataset()
  ckpts = sorted(os.listdir('D:\Semester_6\CS6910\output\q1\checkpoints'))
  latest_ckpt = os.path.join("D:\Semester_6\CS6910\output\q1\checkpoints",
  ckpts[-1])
  print(latest_ckpt)
  state_dict = torch.load(latest_ckpt)['model_state_dict']
  model.load_state_dict(state_dict)
  features = {}
  def get_features(name):
    def hook(model, input, output):
      features[name] = output.detach().cpu()

    return hook
  model[mid_pt].register_forward_hook(get_features('bottleneck'))
  X_inp = torch.tensor(X, dtype=torch.float32).to(device)
  model(X_inp)
  return features['bottleneck'].numpy().astype(np.float32),
  y.astype(np.float32)

def load_PCA_features(num_feat):
  X, y = load_dataset()
  # intialize pca and logistic regression model
  pca = PCA(n_components=num_feat)
  X_std = StandardScaler().fit_transform(X)
  # fit and transform data
  X_pca = pca.fit_transform(X_std)
  return X_pca.astype(np.float32), y.astype(np.float32)

if __name__ == "__main__":
  import shutil
  shutil.rmtree("D:/Semester_6/CS6910/output/q1/checkpoints_classification")
  os.makedirs("D:/Semester_6/CS6910/output/q1/checkpoints_classification")
  shutil.rmtree("D:/Semester_6/CS6910/output/q1/checkpoints")
  os.makedirs("D:/Semester_6/CS6910/output/q1/checkpoints")
  train_AANN(False, num_feat)
  train_classifier(False, num_feat, "AANET")
  train_classifier(False, num_feat, "PCA")