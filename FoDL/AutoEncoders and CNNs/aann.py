# -*- coding: utf-8 -*-
"""AANN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xSpCl5CsoAaxCCAywCYiwxuCRYcNgA7z
"""

from operator import mod
import random
import torch
from torch.utils.data import Dataset , DataLoader
import torch.nn as nn
import os
import numpy as np
import sys
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import pandas as pd
from sklearn.decomposition import PCA
import shutil
torch.manual_seed(3)
torch.cuda.manual_seed_all(3)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(3)
random.seed(3)
os.environ['PYTHONHASHSEED'] = str(3)

num_epochs = 500
batch_size = 4096
size_list = [[48 , 32 , 48] ,
[32 , 24 , 32] ,
[24 , 16 , 24] ]
mid_pt = 1 #len(size_list[0]) // 2 + 1
feat_layer = 1
num_feat = 16 #size_list[-1][len(size_list[-1]) // 2]
patience = 3
learning_rate = float(5e-3)
if torch.cuda.is_available() :
  device = 'cuda'
else :
  device = 'cpu'
save = True

class Dataset(Dataset):
    def __init__(self, X, y):
        super(Dataset, self).__init__()
        self.X = X
        self.y = y

    def __getitem__(self, index):
        sample = {}
        sample["x"] = torch.tensor(self.X[index], dtype=torch.float32)
        sample["y"] = torch.tensor(self.y[index], dtype=torch.float32)
        return sample

    def __len__(self):
        return len(self.X)

def load_dataset():
    # Define the list of classes in the dataset
    classes = ['ragno', 'scoiattolo', 'cane', 'gatto', 'farfalla']

    ds = []
    y = []

    # Load data for each class and concatenate them
    for i, name in enumerate(classes):
        csv_file = os.path.join("datasets", "csv", name + ".csv")
        df = pd.read_csv(csv_file).to_numpy()[:, 1:49]
        ds.append(df)
        y.extend(len(df) * [i])

    # Concatenate all class data and normalize features to the range [0, 1]
    X = np.concatenate(ds, axis=0)
    X = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))

    y = np.reshape(y, [-1, 1])
    return X.astype(np.float32), y.astype(np.float32)

def make_dataloaders(X , y , batch_size = 16 , test_split = 0.2):
  X_train , X_val , Y_train , Y_val = train_test_split(X , y , test_size=
  test_split)
  train_ds = Dataset(X_train , Y_train)
  val_ds = Dataset(X_val,Y_val)
  train_loader = DataLoader(train_ds, batch_size= batch_size,
  shuffle=True , pin_memory=False)
  val_loader = DataLoader(val_ds, batch_size= batch_size,
  shuffle=False , pin_memory= False)
  return train_loader , val_loader

def get_AANET_model(sizes):
    layers = []
    for (inp, out) in zip(sizes[:-1], sizes[1:]):
        layers.append(nn.Linear(inp, out))
        layers.append(nn.ReLU())
    # out = nn.Linear(sizes[-2], sizes[-1])
    # layers.append(out)
    net = nn.Sequential(*layers)
    return net.to(device)

def get_classifier_model(num_feat):
    layers = []
    sizes = [num_feat, 8, 5]
    for (inp, out) in zip(sizes[:-2], sizes[1:]):
        layers.append(nn.Linear(inp, out))
        layers.append(nn.LeakyReLU())
    out = nn.Linear(sizes[-2], sizes[-1])
    layers.append(out)
    net = nn.Sequential(*layers)
    return net.to(device)

def train_AANN(i , X , y , size , resume = False) :
  train_loader , val_loader = make_dataloaders(X, y , batch_size=batch_size)
  model = get_AANET_model(size)
  criterion = nn.MSELoss(reduction = 'mean')
  optimizer = torch.optim.Adam(model.parameters() , lr = learning_rate)
  epoch_start= 0
  if resume :
    ckpts = sorted(os.listdir('output\q2\checkpoints'))
    latest_ckpt = os.path.join("output\q2\checkpoints" , ckpts[-1])
    print(latest_ckpt)
    state_dict = torch.load(latest_ckpt)['model_state_dict']
    model.load_state_dict(state_dict)
    epoch_start = state_dict['epoch']

  avg_losses = []
  avg_losses_val = []
  last_loss = np.Inf
  for epoch in range(epoch_start, num_epochs):
    total_step = len(train_loader)
    model.train()
    sum_loss = 0
    for i, sample in enumerate(train_loader):
      # Move tensors to the configured device
      x = sample['x'].to(device)
      pred_x = model(x)
      loss = criterion(x, pred_x)
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      sum_loss += loss.item()
    avg_loss = sum_loss / total_step
    avg_losses.append(avg_loss)
    with torch.no_grad() :
      model.eval()
      total_step = len(val_loader)
      sum_loss = 0
      for i, sample in enumerate(val_loader):
        # Move tensors to the configured device
        x = sample['x'].to(device)
        # Forward pass
        x_pred = model(x)
        loss = criterion(x , x_pred)
        sum_loss += loss.item()
    avg_loss_val = sum_loss / total_step
    avg_losses_val.append(avg_loss_val)

    if epoch % 50 == 0 :
      torch.save({
      'model_state_dict': model.state_dict(),
      'optimizer_state_dict': optimizer.state_dict(),
      },
      f"./output/q2/checkpoints/ckpt_AANET_{i}_{epoch}.pth")
      print('EPOCH : {} Loss(Train) of the network {}'.format(epoch
      ,avg_loss))
      print('EPOCH : {} Loss(Val) of the network {}'.format(epoch
      ,avg_loss_val))
    if avg_loss_val > last_loss:
      trigger_times += 1
      print('Trigger Times:', trigger_times)
      print('EPOCH : {} Loss(Train) of the network {}'.format(epoch
      ,avg_loss))
      print('EPOCH : {} Loss(Val) of the network {}'.format(epoch
      ,avg_loss_val))
      if trigger_times >= patience:
        print('Early stopping!')
        torch.save({
        'epoch' : epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        }, f"./output/q1/checkpoints/ckpt_{epoch}.pth")
        break


    else:
      trigger_times = 0
    last_loss = avg_loss_val

  plt.plot(avg_losses_val)
  plt.plot(avg_losses)
  plt.show()
  plt.clf()
  features = {}
  def get_features(name):
    def hook(model, input, output):
      features[name] = output.detach().cpu
    return hook