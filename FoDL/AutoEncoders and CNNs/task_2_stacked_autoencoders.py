# -*- coding: utf-8 -*-
"""Task 2 Stacked Autoencoders

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10QVeB7bmS4RRW1XTU7cNLrNbo22Tfb4z
"""

from operator import mod
import random
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import os
import numpy as np
import sys
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
import pandas as pd
from sklearn.decomposition import PCA
torch.manual_seed(3)
torch.cuda.manual_seed_all(3)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(3)
random.seed(3)
os.environ['PYTHONHASHSEED'] = str(3)

"""
Hyper Parameters
"""
num_epochs = 500
batch_size = 4096
size_list = [[48 , 32 , 48] ,
[32 , 24 , 32] ,
[24 , 16 , 24] ]
mid_pt = 1 #len(size_list[0]) // 2 + 1
feat_layer = 1
num_feat = 16 #size_list[-1][len(size_list[-1]) // 2]
#mid_pt = len(size_list[0]) // 2 + 1
#num_feat = size_list[-1][len(size_list[0]) // 2]
patience = 3
learning_rate = float(5e-3)
if torch.cuda.is_available():
  device = 'cuda'
else:
  device = 'cpu'
save = True

class Dataset(Dataset):
    def __init__(self, X, y):
        super(Dataset, self).__init__()
        self.X = X
        self.y = y

    def __getitem__(self, index):
        sample = {}
        sample["x"] = torch.tensor(self.X[index], dtype=torch.float32)
        sample["y"] = torch.tensor(self.y[index], dtype=torch.float32)
        return sample

    def __len__(self):
        return len(self.X)

def load_dataset():
    # Define the list of classes in the dataset
    classes = ['ragno', 'scoiattolo', 'cane', 'gatto', 'farfalla']

    ds = []
    y = []

    # Load data for each class and concatenate them
    for i, name in enumerate(classes):
        csv_file = os.path.join("D:/Semester_6/CS6910", "csv", name + ".csv")
        df = pd.read_csv(csv_file).to_numpy()[:, 1:49]
        ds.append(df)
        y.extend(len(df) * [i])

    # Concatenate all class data and normalize features to the range [0, 1]
    X = np.concatenate(ds, axis=0)
    X = (X - np.min(X, axis=0)) / (np.max(X, axis=0) - np.min(X, axis=0))

    y = np.reshape(y, [-1, 1])
    return X.astype(np.float32), y.astype(np.float32)

def make_dataloaders(X, y, batch_size=16, test_split=0.2):
  X_train, X_val, Y_train, Y_val = train_test_split(X, y,
  test_size=test_split)
  train_ds = Dataset(X_train, Y_train)
  val_ds = Dataset(X_val, Y_val)
  train_loader = DataLoader(train_ds, batch_size=batch_size,
  shuffle=True, pin_memory=False)
  val_loader = DataLoader(val_ds, batch_size=batch_size,
  shuffle=False, pin_memory=False)
  return train_loader, val_loader

def get_AANET_model(sizes):
  layers = []
  for (inp, out) in zip(sizes[:-1], sizes[1:]):
    layers.append(nn.Linear(inp, out))
    layers.append(nn.ReLU())
  # out = nn.Linear(sizes[-2] , sizes[-1])
  # layers.append(out)
  net = nn.Sequential(*layers)
  return net.to(device)

def get_classifier_model(num_feat):
  layers = []
  sizes = [num_feat, 8, 5]
  for (inp, out) in zip(sizes[:-2], sizes[1:]):
    layers.append(nn.Linear(inp, out))
    layers.append(nn.LeakyReLU())
  out = nn.Linear(sizes[-2], sizes[-1])
  layers.append(out)
  net = nn.Sequential(*layers)
  return net.to(device)

def train_AANN(i, X, y, size, resume=False):
  train_loader, val_loader = make_dataloaders(X, y, batch_size=batch_size)
  model = get_AANET_model(size)
  criterion = nn.MSELoss(reduction='mean')
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  epoch_start = 0
  if resume:
    latest_ckpt = os.path.join("D:\Semester_6\CS6910\output\q2\checkpoints",
ckpts[-1])
    print(latest_ckpt)
    state_dict = torch.load(latest_ckpt)['model_state_dict']
    model.load_state_dict(state_dict)
    epoch_start = state_dict['epoch']
  avg_losses = []
  avg_losses_val = []
  last_loss = np.Inf
  for epoch in range(epoch_start, num_epochs):
    total_step = len(train_loader)
    model.train()
    sum_loss = 0
    for i, sample in enumerate(train_loader):
      # Move tensors to the configured device
      x = sample['x'].to(device)
      pred_x = model(x)
      loss = criterion(x, pred_x)
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      sum_loss += loss.item()
    avg_loss = sum_loss / total_step
    avg_losses.append(avg_loss)

    with torch.no_grad():
      model.eval()
      total_step = len(val_loader)
      sum_loss = 0
      for i, sample in enumerate(val_loader):
      # Move tensors to the configured device
        x = sample['x'].to(device)
        # Forward pass
        x_pred = model(x)
        loss = criterion(x, x_pred)
        sum_loss += loss.item()
    avg_loss_val = sum_loss / total_step
    avg_losses_val.append(avg_loss_val)

    if epoch % 50 == 0:
      torch.save({
      'model_state_dict': model.state_dict(),
      'optimizer_state_dict': optimizer.state_dict(),
 8     },
      f"D:/Semester_6/CS6910/output/q2/checkpoints/ckpt_AANET_{i}_{epoch}.pth")
      print('EPOCH : {} Loss(Train) of the network {}'.format(epoch,
      avg_loss))
      print('EPOCH : {} Loss(Val) of the network {}'.format(epoch,
      avg_loss_val))
    if avg_loss_val > last_loss:
      trigger_times += 1
      print('Trigger Times:', trigger_times)
      print('EPOCH : {} Loss(Train) of the network {}'.format(epoch,
      avg_loss))
      print('EPOCH : {} Loss(Val) of the network {}'.format(epoch,
      avg_loss_val))
      if trigger_times >= patience:
        print('Early stopping!')
        torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        },
        f"D:/Semester_6/CS6910/output/q1/checkpoints/ckpt_{epoch}.pth")
        break
    else:
      trigger_times = 0
    last_loss = avg_loss_val
  plt.plot(avg_losses_val)
  plt.plot(avg_losses)
  plt.savefig(f"D:/Semester_6/CS6910/output/q2/AANET_{i}_loss.png")
  plt.clf()
  features = {}
  def get_features(name):
    def hook(model, input, output):
      features[name] = output.detach().cpu()
    return hook
  model[mid_pt].register_forward_hook(get_features('bottleneck'))
  X_inp = torch.tensor(X, dtype=torch.float32).to(device)
  model(X_inp)
  return (features['bottleneck'].numpy().astype(np.float32),y.astype(np.float32)), model[:feat_layer]

def train_classifier(model, X, y):
  train_loader, val_loader = make_dataloaders(X=X, y=y, batch_size=batch_size)
  criterion = nn.CrossEntropyLoss(reduction='mean')
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  epoch_start = 0
  avg_losses = []
  avg_losses_val = []
  avg_acc = []
  last_loss = np.Inf

  for epoch in range(epoch_start + 1, num_epochs + 1):
    total_step = len(train_loader)
    model.train()
    sum_loss = 0
    for i, sample in enumerate(train_loader):
      # Move tensors to the configured device
      x = sample['x'].to(device)
      y = sample['y'].to(device).long().squeeze(1)
      pred_x = model(x)
      loss = criterion(pred_x, y)
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      sum_loss += loss.item()
    avg_loss = sum_loss / total_step
    print('Loss(Train) of the network after {} epochs {}'.format(epoch,
    avg_loss))
    avg_losses.append(avg_loss)
    with torch.no_grad():
      model.eval()
      total_step = len(val_loader)
      sum_loss, total, correct = 0, 0, 0
      for i, sample in enumerate(val_loader):
        # Move tensors to the configured device
        x = sample['x'].to(device)
        y = sample['y'].to(device).long().squeeze(1)
        # Forward pass
        x_pred = model(x)
        loss = criterion(x_pred, y)
        sum_loss += loss.item()
        _, predicted = torch.max(x_pred.data, 1)
        total += y.size(0)
        correct += (predicted == y).sum().item()
    acc = 100 * correct / total
    print('Accuracy of the network after {} epochs {} %'.format(epoch, acc))
    avg_acc.append(acc)
    avg_loss_val = sum_loss / total_step
    print('Loss(Val) of the network after {} epochs {}'.format(epoch,
    avg_loss_val))
    avg_losses_val.append(avg_loss_val)
    torch.save({'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    },
    f"D:/Semester_6/CS6910/output/q1/checkpoints/ckpt_classification_{epoch}.pth")
    if avg_loss_val > last_loss:
      trigger_times += 1
      print('Trigger Times:', trigger_times)
      if trigger_times >= patience:
        print('Early stopping!')
        torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        },
        f"D:/Semester_6/CS6910/output/q1/checkpoints_classification/ckpt_{epoch}.pth")
        break
    else:
      trigger_times = 0

    last_loss = avg_loss_val

  plt.plot(avg_losses_val)
  plt.plot(avg_losses)
  plt.savefig(f"D:/Semester_6/CS6910/output/q2/classification_loss.png")
  plt.clf()
  return model

if __name__ == "__main__":
  import shutil
  shutil.rmtree("D:/Semester_6/CS6910/output/q2/checkpoints_classification")
  os.makedirs("D:/Semester_6/CS6910/output/q2/checkpoints_classification")
  shutil.rmtree("D:/Semester_6/CS6910/output/q2/checkpoints")
  os.makedirs("D:/Semester_6/CS6910/output/q2/checkpoints")
  models = []
  feat, y = load_dataset()
  for i, size in enumerate(size_list):
    print(f"Unsupervised Pretraining AANET : {i + 1}")
    (feat, y), model = train_AANN(i, feat, y, size, False)
    models.append(model)
  output_layer = nn.Linear(size_list[-1][1], 5)
  models.append(output_layer)
  print(f"Supervised Training")
  feat, y = load_dataset()
  DNN_classifier = nn.Sequential(*models).to(device)
  train_classifier(DNN_classifier, feat, y)



