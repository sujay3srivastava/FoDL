# -*- coding: utf-8 -*-
"""MLFFNNClassifier

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19YGkFmFDZhgR70tbcNxCrUA6-WDceUUH
"""

from importlib.machinery import OPTIMIZED_BYTECODE_SUFFIXES
import random
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import os
import numpy as np
import sys
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sn
import pandas as pd
from sklearn.metrics import confusion_matrix
torch.manual_seed(3)
torch.cuda.manual_seed_all(3)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
np.random.seed(3)
random.seed(3)
os.environ['PYTHONHASHSEED'] = str(3)

"""
Hyper Parameters
"""
input_size = 60
hidden_size1 = 120
hidden_size2 = 30
num_classes = 5
num_epochs = 10
batch_size = 16
learning_rate = float(1e-2)
test_split = 0.3
optim = "generalized delta"
thresh = float(1e-2)
if torch.cuda.is_available():
  device = 'cuda'
else:
  device = 'cpu'
save = True

def make_dataloaders(batch_size=16, test_split=0.3, data_dir="./datasets"):
    # Set the data directory path
    data_dir = data_dir

    # Read the image data and labels from files
    images = open(os.path.join(data_dir, "task2b", "image_data_dim60.txt"), "r")
    images = images.read().strip().split()
    images = np.array([images[60 * i: 60 * (i + 1)] for i in range(2688)]).astype(np.float32)

    labels = open(os.path.join(data_dir, "task2b", "image_data_labels.txt"), "r")
    labels = labels.read().strip().split("\n")
    labels = np.array(labels).astype(int)

    # Filter valid indices based on certain labels
    valid_idxs = np.where(
        (labels == 0)
        | (labels == 2)
        | (labels == 4)
        | (labels == 6)
        | (labels == 7)
    )

    # Split the data into train and validation sets
    train_images = images[valid_idxs]
    train_labels = labels[valid_idxs]
    X_train, X_val, Y_train, Y_val = train_test_split(train_images, train_labels, test_size=test_split)

    # Create train and validation datasets
    train_ds = ClassificationDataset(mode='train', X=X_train, y=Y_train)
    val_ds = ClassificationDataset(mode='val', X=X_val, y=Y_val)

    # Create train and validation data loaders
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=False)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=False)

    return train_loader, val_loader

class NeuralNet(nn.Module):
  def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):
    super(NeuralNet, self).__init__()
    self.fc1 = nn.Linear(input_size, hidden_size1)
    self.tanh = nn.Tanh()
    self.fc2 = nn.Linear(hidden_size1, hidden_size2)
    self.tanh = nn.Tanh()
    self.out = nn.Linear(hidden_size2, num_classes)
    def forward(self, x):
      out = self.fc1(x)
      out = self.tanh(out)
      out = self.fc2(out)
      out = self.tanh(out)
      out = self.out(out)
      return out

def train(model, optimizer, criterion, train_loader, val_loader):
    avg_losses = []
    avg_acc = []

    for epoch in range(num_epochs):
        total_step = len(train_loader)
        sum_loss = 0
        model.train()

        # Training loop
        for i, sample in enumerate(train_loader):
            # Move tensors to the configured device
            images = sample['img'].to(device)
            labels = sample['label'].to(device)
            class_name = sample['class_name']

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            sum_loss += loss.item()

        avg_loss = sum_loss / total_step
        print('Loss of the network after {} epochs: {}'.format(epoch, avg_loss))
        avg_losses.append(avg_loss)

        with torch.no_grad():
            model.eval()
            total, correct = 0, 0

            # Validation loop
            for i, sample in enumerate(val_loader):
                # Move tensors to the configured device
                images = sample['img'].to(device)
                labels = sample['label'].to(device)
                class_name = sample['class_name']

                # Forward pass
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)

                total += labels.size(0)
                correct += (predicted == labels).sum().item()

            acc = 100 * correct / total
            print('Accuracy of the network after {} epochs: {} %'.format(epoch, acc))
            avg_acc.append(acc)

            # Check if the model converged
            if (avg_acc[-1] - avg_acc[-2]) / avg_acc[-2] < thresh:
                print("Model converged!")
                print(f"Epochs taken: {epoch}")

            # Save training progress if specified
            if save:
                np.save(f"./output/avg_acc_{optim}.npy", avg_acc)
                np.save(f"./output/avg_loss_{optim}.npy", avg_losses)
                torch.save({
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                }, f"./output/ckpt_{optim}.pth")

def plot_confmat(optim, loader):
    for optim in optims:
        model = NeuralNet(input_size, hidden_size1, hidden_size2, num_classes).to(device)
        state_dict = torch.load(f"./output/q2b/checkpoints/ckpt_{optim}.pth")
        model.load_state_dict(state_dict['model_state_dict'])
        model.eval()
        y_true = []
        y_pred = []

        for i, sample in enumerate(loader):
            # Move tensors to the configured device
            images = sample['img'].to(device)
            labels = sample['label']
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            y_pred.extend(predicted.cpu().numpy())
            y_true.extend(labels.numpy())

        conf_mat = confusion_matrix(y_pred=y_pred, y_true=y_true)
        df_cm = pd.DataFrame(conf_mat, index=[i for i in ["coast", "highway", "mountain", "street", "tallbuilding"]],
                             columns=[i for i in ["coast", "highway", "mountain", "street", "tallbuilding"]])
        plt.figure(figsize=(10, 7))
        plt.title(f"Confusion Matrix : {optim}")
        sn.heatmap(df_cm, annot=True)
        plt.savefig(f"./output/q2b/images/conf_mat_{optim}_train.png")

def plot_error(optims):
    acc = [np.load(f"./output/q2b/avg_acc_{optim}.npy") for optim in optims]
    loss = [np.load(f"./output/q2b/avg_loss_{optim}.npy") for optim in optims]

    # Plot accuracy for different optimizers
    for a in acc:
        plt.plot(a)
    plt.xlabel("Epochs")
    plt.ylabel("Accuracy")
    plt.legend(["delta", "adam", "momentum : 0.1", "momentum : 0.5", "momentum: 0.9"])
    plt.show()

    # Plot loss for different optimizers
    for l in loss:
        plt.plot(l)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend(["delta", "adam", "momentum : 0.1", "momentum : 0.5", "momentum: 0.9"])
    plt.show()

if __name__ == "__main__":
    # Create the neural network model
    model = NeuralNet(input_size, hidden_size1, hidden_size2, num_classes).to(device)

    # Get the train and validation data loaders
    train_loader, val_loader = make_dataloaders(batch_size=batch_size, test_split=test_split)

    # Uncomment and use these lines for setting up loss and optimizer
    # criterion = nn.CrossEntropyLoss(reduction='mean')
    # if optim == "adam":
    #     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    # elif optim == "delta":
    #     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0)
    # elif optim == "generalized delta":
    #     momentums = [0.1, 0.5, 0.9]
    #     for momentum in momentums:
    #         optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    #     train(model, optimizer, criterion)

    # List of optimizers to be tested
    optims = ["adam", "delta", "generalized delta_0.1", "generalized delta_0.5", "generalized delta_0.9"]

    # Uncomment and use these lines for plotting confusion matrix and error
    # plot_confmat(optims, train_loader)
    # plot_error(optims)

