# -*- coding: utf-8 -*-
"""FunctionApproximation:2d Input

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zJVbWBNMTys250SGgVAYyPY_dNnzk7U3
"""

import random
import torch
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import os
import numpy as np
import sys
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import pandas as pd

input_size = 2
hidden_size1 = 10
hidden_size2 = 5
num_out = 1
num_epochs = 100
batch_size = 16
learning_rate = float(1e-3)
if torch.cuda.is_available():
  device = 'cuda'
else:
  device = 'cpu'
save = True

class Dataset(Dataset):
  def __init__(self, mode, X, y):
    super(Dataset, self).__init__()
    self.X = X
    self.y = y
  def __getitem__(self, index):
    sample = {}
    sample["x"] = torch.tensor(self.X[index], dtype=torch.float32)
    sample["y"] = torch.tensor(self.y[index],
    dtype=torch.float32).unsqueeze(0)
    return sample
  def __len__(self):
    return len(self.X)

def make_dataloaders(batch_size=16, test_split=0.3):
  df = pd.read_csv("/content/func_app1.csv")
  y = df["y"].to_numpy()
  X = df[["x1", "x2"]].to_numpy()
  X_train, X_val, Y_train, Y_val = train_test_split(X, y,
  test_size=test_split)
  train_ds = Dataset('train', X_train, Y_train)
  val_ds = Dataset('val', X_val, Y_val)
  train_loader = DataLoader(train_ds, batch_size=batch_size,
  shuffle=True, num_workers=2,
  pin_memory=False)
  val_loader = DataLoader(val_ds, batch_size=batch_size,
  shuffle=False, num_workers=2,
  pin_memory=False)
  return train_loader, val_loader

class NeuralNet(nn.Module):
  def __init__(self, input_size, hidden_size1, hidden_size2, num_out):
    super(NeuralNet, self).__init__()
    self.fc1 = nn.Linear(input_size, hidden_size1)
    self.tanh = nn.Tanh()
    self.fc2 = nn.Linear(hidden_size1, hidden_size2)
    self.tanh = nn.Tanh()
    self.out = nn.Linear(hidden_size2, num_out)
  def forward(self, x):
    x = self.fc1(x)
    x = self.tanh(x)
    x = self.fc2(x)
    x = self.tanh(x)
    x = self.out(x)
    return x
def train(model, optimizer, criterion, train_loader, val_loader):
# model = NeuralNet(input_size, hidden_size1, hidden_size2, num_out).to(device)
# state_dict = torch.load(f"./output/q1/ckpt_40.pth")
# model.load_state_dict(state_dict['model_state_dict'])
  avg_losses = []
  avg_losses_val = []
  for epoch in range(num_epochs):
    total_step = len(train_loader)
    sum_loss = 0
    model.train()
    for i, sample in enumerate(train_loader):
      # Move tensors to the configured device
      x = sample['x'].to(device)
      y = sample['y'].to(device)
      # Forward pass
      outputs = model(x)
      loss = criterion(outputs, y)
      # Backward and optimize
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      sum_loss += loss.item()
      avg_loss = sum_loss / total_step
      print('Loss(Train) of the network after {} epochs {}'.format(epoch,
      avg_loss))
      avg_losses.append(avg_loss)
    with torch.no_grad():
      model.eval()
      total_step = len(val_loader)
      sum_loss = 0
      for i, sample in enumerate(val_loader):
    # Move tensors to the configured device
        x = sample['x'].to(device)
        y = sample['y'].to(device)
        # Forward pass
        outputs = model(x)
        loss = criterion(outputs, y)
        sum_loss += loss.item()
    avg_loss_val = sum_loss / total_step
    print('Loss(Val) of the network after {} epochs {}'.format(epoch,
    avg_loss_val))
    avg_losses_val.append(avg_loss_val)
    # if save :
    # torch.save({
    # 'model_state_dict': model.state_dict(),
    # 'optimizer_state_dict': optimizer.state_dict(),
    # }, f"./output/q2a/ckpt_{epoch}.pth")
    if epoch % 20 == 0:
      torch.save({
      'model_state_dict': model.state_dict(),
      'optimizer_state_dict': optimizer.state_dict(),
      }, f"./ckpt_{epoch}.pth")
    np.save("./val_loss.npy", avg_losses_val)
    np.save("./train_loss.npy", avg_losses)

def predict(input, epoch=80):
  model = NeuralNet(input_size, hidden_size1, hidden_size2,
  num_out).to(device)
  state_dict = torch.load(f"./ckpt_{epoch}.pth")
  model.load_state_dict(state_dict['model_state_dict'])
  input = torch.tensor(input, dtype=torch.float32).to(device)
  with torch.no_grad():
    model.eval()
    out = model(input)
  return out.detach().cpu().numpy()

def plot_loss():
  loss_val = np.load("./val_loss.npy")
  loss_train = np.load("./train_loss.npy")
  plt.plot(loss_train)
  plt.plot(loss_val)
  plt.xlabel("Epochs")
  plt.ylabel("Loss")
  plt.legend(['train', 'val'])
  plt.title("Loss vs Epoch")
  plt.show()
def plot3d():
  df = pd.read_csv("/content/func_app1.csv")
  X = df[['x1', 'x2']].to_numpy()
  y_true = df['y'].to_numpy()
  x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
  y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
  x_surf, y_surf = np.meshgrid(np.linspace(x_min, x_max, 20),
  np.linspace(y_min, y_max, 20))
  df = np.c_[x_surf.ravel(), y_surf.ravel()]
  y_pred = predict(df)
  fig = plt.figure()
  ax = fig.add_subplot(111, projection='3d')
  ax.scatter(X[:, 0], X[:, 1], y_true, c='red', marker='o', alpha=0.5)
  ax.plot_surface(x_surf, y_surf, y_pred.reshape(x_surf.shape), color='b',
  alpha=0.3)
  ax.set_xlabel('x1')
  ax.set_ylabel('x2')
  ax.set_zlabel('y')
  plt.show()

if __name__ == "__main__":
  train_loader, val_loader = make_dataloaders(batch_size=batch_size)
  model = NeuralNet(input_size, hidden_size1, hidden_size2,
  num_out).to(device)
  criterion = nn.MSELoss(reduction='mean')
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
  # optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate ,momentum = 0.5)
  train(model, optimizer, criterion, train_loader, val_loader)
  plot_loss()
  plot3d()