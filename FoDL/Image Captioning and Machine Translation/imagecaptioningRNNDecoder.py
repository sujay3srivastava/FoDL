# -*- coding: utf-8 -*-
"""ImageCaptioning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F9LtuNGd6iHyFskTBAc4-icPb5UYxex1
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image
import spacy
from tqdm import tqdm
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from torch.nn.utils.rnn import pad_sequence
import torchvision.models as models
import torch.optim as optim
from nltk.translate.bleu_score import sentence_bleu
from spacy.lang.en.examples import sentences
device = "cuda" if torch.cuda.is_available() else "cpu"

device = "cuda" if torch.cuda.is_available() else "cpu"
img_dir = "Images/"
transform = transforms.Compose(
[transforms.Resize(256), transforms.ToTensor(),
transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224,
0.225])]
)

with open("19-20220501T145027Z-001/19/captions.txt") as f:
  images_captions = f.readlines()
for i in range(len(images_captions)):
  image_captions = images_captions[i]
  image_name = image_captions.split('#')[0]
  caption = image_captions.split('\t')[1][:-1]
  if os.path.exists("Images/" + image_name):
    images_captions[i] = (image_name, caption)
  else:
    images_captions[i] = -1
images_captions = [image_captions for image_captions in images_captions if
image_captions != -1]
spacy_eng = spacy.load("en_core_web_sm")

vocab, embeddings = [], []
with open('glove.6B/glove.6B.50d.txt', 'rt') as fi:
  full_content = fi.read().strip().split('\n')
for i in range(len(full_content)):
  i_word = full_content[i].split(' ')[0]
  vocab.append(i_word)
  i_embeddings = [float(val) for val in full_content[i].split(' ')[1:]]
  embeddings.append(i_embeddings)

embs_npa = np.array(embeddings)
pad_emb_npa = np.zeros((1, embs_npa.shape[1]))
sos_emb_npa = np.ones((1, embs_npa.shape[1]))
eos_emb_npa = np.full((1, embs_npa.shape[1]), 185)
unk_emb_npa = np.mean(embs_npa, axis=0, keepdims=True)
embs_npa = np.vstack((pad_emb_npa, sos_emb_npa, eos_emb_npa, unk_emb_npa,
embs_npa))
BATCH_SIZE = 8

class Vocab:
    def __init__(self, vocab=vocab):
        self.vocab = vocab
        self.itos = {
            0: "<PAD>",
            1: "<SOS>",
            2: "<EOS>",
            3: "<UNK>"
        }
        self.stoi = {
            "<PAD>": 0,
            "<SOS>": 1,
            "<EOS>": 2,
            "<UNK>": 3
        }

    def __len__(self):
        return len(self.itos)

    def tokenizer_eng(text):
        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]

    def _build_vocabulary(self):
        idx = 4
        for word in self.vocab:
            self.stoi[word] = idx
            self.itos[idx] = word
            idx += 1

    def numericalize(self, text):
        tokenized_text = self.tokenizer_eng(text)
        return [
            self.stoi[token] if token in self.stoi else self.stoi["<UNK>"] for token in tokenized_text
        ]

class ImageCaptionDataset(Dataset):
  def __init__(self, images_captions, root_dir=img_dir, transform=transform,
  freq_threshold=5):
    self.images_captions = images_captions
    self.root_dir = root_dir
    self.transform = transform
    self.vocab = Vocab()
  def len(self):
    return len(self.images_captions)
  def getitem(self, idx):
    image_captions = self.images_captions[idx]
    img_path = os.path.join(self.root_dir, image_captions[0])
    image = Image.open(img_path)
    if self.transform:
      image = self.transform(image)
    caption = image_captions[1]
    numericalised_caption = [self.vocab.stoi["<SOS>"]]
    numericalised_caption += self.vocab.numericalize(caption)
    numericalised_caption += [self.vocab.stoi["<EOS>"]]
    print("var1 =", image)
    print("var2 =", torch.tensor(numericalised_caption))
    return image, torch.tensor(numericalised_caption)

dataset = ImageCaptionDataset(images_captions)

class Collate_Fn:
  def __init__(self, pad_idx):
    self.pad_idx = pad_idx
  def call(self, batch):
    imgs = [item[0].unsqueeze(0) for item in batch]
    imgs = torch.cat(imgs, dim=0)
    targets = [item[1] for item in batch]
    targets = pad_sequence(targets, batch_first=False,
    padding_value=self.pad_idx)
    targets = targets[:, :16]
    return imgs, targets

data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,
collate_fn=Collate_Fn(pad_idx=0))

class NetVLAD(nn.Module):
  def __init__(self, D=512, k=16): # Hyperparameters are D and k
    super(NetVLAD, self).__init__()
    self.D = D
    self.k = k
    self.conv = nn.Sequential(nn.Conv2d(D, k, kernel_size=1, bias=True),
    nn.Softmax(dim=1))
    self.centroids = nn.Parameter(torch.rand(k, D))
    self.flatten = nn.Flatten()
  def forward(self, x):
    x = x.reshape(-1, self.D, 7, 7)
    a = self.conv(x)
    z = torch.sum(
    a.reshape(-1, 1, 7 * 7, self.k) * (
    x.reshape(-1, self.D, 7 * 7, 1) - self.centroids.reshape(1,
    self.D, 1, self.k)),
    dim=2).transpose(2, 1)
    return self.flatten(z)

# CNN
class EncoderCNN(nn.Module):
  def __init__(self, D=512, k=16, embed_size=100):
    super(EncoderCNN, self).__init__()
    self.cnn = models.vgg16(pretrained=True)
    self.cnn.classifier = NetVLAD(D=D, k=k)
    for name, param in self.cnn.named_parameters():
      if "features" in name:
        param.requires_grad = False
      else:
        param.requires_grad = True
    self.embed = nn.Linear(in_features=D * k, out_features=embed_size)
  def forward(self, x):
    y = self.cnn(x)
    z = self.embed(y)
    return z

vocab_size = dataset.vocab

# RNN
class DecoderRNN(nn.Module):
  def __init__(self, embed_size=100, hidden_size=256, vocab_size=vocab_size,
  num_layers=1, max_seq_len=20):
    super(DecoderRNN, self).__init__()
    self.max_seq_len = max_seq_len
    self.embed_size = embed_size
    self.hidden_size = hidden_size
    self.vocab_size = vocab_size
    self.embed =
    nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())
    self.embed.weight.requires_grad = False
    self.rnn = nn.RNN(input_size=embed_size, hidden_size=hidden_size,
    num_layers=num_layers)
    # self.linear = nn.Linear(in_features=hidden_size,
    out_features=vocab_size)
    self.softmax = nn.Softmax(dim=1)
    self.dropout = nn.Dropout(0.3)
  def forward(self, features, captions):
    embeddings = self.embed(captions)
    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)
    hiddens, _ = self.rnn(embeddings)
    outputs = self.linear(hiddens)
    return outputs

class EncoderDecoder(nn.Module):
  def __init__(self, D=512, k=16, embed_size=50, hidden_size=256,
  num_layers=1): # vocab_size=vocab_size,
    super(EncoderDecoder, self).__init__()
    self.encoder = EncoderCNN(D, k, embed_size)
    self.decoder = DecoderRNN(embed_size, hidden_size, num_layers) # vocab_size,
  def forward(self, images, captions):
    features = self.encoder(images)
    outputs = self.decoder(features, captions)
    return outputs
  def caption_image(self, image, vocabulary, max_length=50):
    result_caption = []
    with torch.no_grad():
      x = self.encoder(image).unsqueeze(0)
      states = None
      for _ in range(max_length):
        hiddens, states = self.decoder.rnn(x, states)
        output = self.decoder.linear(hiddens.squeeze(0))
        predicted = output.argmax(1)
        result_caption.append(predicted.item())
        x = self.decoder.embed(predicted).unsqueeze(0)
        if vocabulary.itos[predicted.item()] == "<EOS>":
          break
    return [vocabulary.itos[idx] for idx in result_caption]

num_epochs = 10
learning_rate = 5e-4
print("Reached")
model = EncoderDecoder().to(device)
print(model)
criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi["<PAD>"])
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
print("Training")
model.train()
for epoch in range(num_epochs):
  log_loss = []
  for idx, (imgs, captions) in tqdm(enumerate(data_loader), total=len([data_loader]), leave=False):
    imgs = imgs.to(device)
    captions = captions.to(device)
    outputs = model(imgs, captions[:-1])
    loss = criterion(outputs.reshape(-1, outputs.shape[2]),
    captions.reshape(-1))
    log_loss.append(loss.item())
    optimizer.zero_grad()
    loss.backward(loss)
    optimizer.step()

  torch.save(model.state_dict(), f'epoch-{epoch}-model.pt')
  torch.save(optimizer.state_dict(), f'epoch-{epoch}-optimizer.pt')
  print(f'Average Training Error {epoch}:', sum(log_loss) / len(log_loss))

fig = plt.figure(figsize=(26, 26))
for i, k in enumerate([56, 2356, 25423, 111]):
  image_name = images_captions[k][0]
  caption = images_captions[k][1]
  test_img = transform(Image.open(img_dir + image_name)).unsqueeze(0)
  predicted_caption = " ".join(model.caption_image(test_img.to(device),
  dataset.vocab))
  image = Image.open(img_dir + image_name)
  image = transforms.ToTensor()(image).cpu().numpy()
  image = image.transpose((1, 2, 0))
  plt.subplot(4, 1, i + 1)
  plt.imshow(image)
  plt.title("Actual: " + caption + '\n' + "Predicted: " + predicted_caption)
  plt.axis("off")

plt.tight_layout()
plt.show()