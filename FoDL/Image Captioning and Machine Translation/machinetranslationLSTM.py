# -*- coding: utf-8 -*-
"""MachineTranslationLSTM

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ouf6V2jc4WRbCjsV7vTzuk6ReB1X2wlH
"""

from torchtext.data import Field, BucketIterator, TabularDataset, Iterator
from transformers import AutoModel, AutoTokenizer
from torch.utils.tensorboard import SummaryWriter
import torch.optim as optim
import torch.nn as nn
import torchtext.vocab as vocab
from torchtext.data.metrics import bleu_score
import numpy as np
import pandas as pd
import torch
import spacy
# from string import digits
# import string
# import re
# Tensorboard to get nice loss plot
writer = SummaryWriter(f"runs/loss_plot")
step1 = 0
step2 = 0
# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/en-ta/

#Train
en_data = open("./en-ml/train.en", encoding='utf-8').read().split('\n')
ml_data = open("./en-ml/train.ml", encoding='utf-8').read().split('\n')
#en_data, ta_data = en_data[:len(en_data)//10], ta_data[:len(ta_data)//10]
#len(en_data), len(ta_data)
train_data = {'en': [text for text in en_data], 'ml': [text for text in ml_data]}
df_train = pd.DataFrame(train_data, columns=['en', 'ml'])
#len(df_train)
df_train['eng_len'] = df_train['en'].str.count(' ')
df_train['ml_len'] = df_train['ml'].str.count(' ')
df_train = df_train.query('ml_len < 80 & eng_len < 80')
df_train = df_train.query('ml_len < eng_len * 1.5 & ml_len * 1.5 > eng_len')
print(len(df_train))
df_train = df_train.iloc[: int(0.1 * len(df_train))]
df_train.to_csv('train2.csv', index=False)
print(len(df_train))
# Validation
dev_en_data = open("dev.en", encoding='utf-8').read().split('\n')
dev_ml_data = open("dev.ml", encoding='utf-8').read().split('\n')
val_data = {'en': [text for text in dev_en_data], 'ta': [text for text in dev_ml_data]}
df_val = pd.DataFrame(val_data, columns=['en', 'ta'])
# df_val = df_val.iloc[:10]
df_val.to_csv('dev2.csv', index=False)
print(len(df_val))
# Test
test_en_data = open("test.en", encoding='utf-8').read().split('\n')
test_ml_data = open("test.ml", encoding='utf-8').read().split('\n')
test_data = {'en': [text for text in test_en_data], 'ml': [text for text in test_ml_data]}
df_test = pd.DataFrame(test_data, columns=['en', 'ml'])
# df_test = df_test.iloc[:10]
df_test.to_csv('test2.csv', index=False)
print(len(df_test))
# vocab_to_embedding = vocab.GloVe(name='6B', dim=50)
# x = vocab_to_embedding.vectors
# x.shape, x
print('Preparing ml tokenizers....')
# ml preprocessors
tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')
model = AutoModel.from_pretrained('ai4bharat/indic-bert')
vocab_to_embedding_converter = model.get_input_embeddings()

def tokenize_ml(text):
  return tokenizer.tokenize(text)
print('Preparing en tokenizers...')
# en preprocessors
spacy_eng = spacy.load("en_core_web_sm")
def tokenize_en(text):
  return [token.text for token in spacy_eng.tokenizer(text)]

en = Field(sequential=True, use_vocab=True, tokenize=tokenize_en, lower=True,
init_token="<sos>", eos_token="<eos>")
ml = Field(sequential=True, use_vocab=True, tokenize=tokenize_ml, lower=True,
init_token="<sos>", eos_token="<eos>")
fields = [("en", en), ("ml", ml)]
print('Preparing dataloader....')
train_data, val_data, test_data = TabularDataset.splits(path='./', train='train2.csv',
validation='dev2.csv', test='test2.csv' , format="csv", fields=fields, skip_header=True )
print('Building Vocabulary....')
en.build_vocab(train_data, max_size=50000, min_freq=2, vectors="glove.6B.100d")
ml.build_vocab(train_data, max_size=50000, min_freq=2)
print(len(en.vocab))
print(len(ml.vocab))

# Hyperparameters
epochs = 100
learning_rate = 1e-3
batch_size = 32
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
input_size_encoder = len(en.vocab)
input_size_decoder = 20000
output_size = len(ml.vocab)
encoder_embedding_size = 100
decoder_embedding_size = 128
hidden_size = 1024 # Needs to be the same for both RNN's
num_layers = 1
train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, val_data, test_data),
batch_size=batch_size, device=device , sort = False )
print(next(iter(valid_iterator)))

class Encoder(nn.Module):
  def __init__(self, input_size, emb_size, hidden_size, num_layers):
    super(Encoder, self).__init__()
    self.hidden_size = hidden_size
    vocab = en.vocab
    self.embedding = nn.Embedding(input_size, emb_size)
    self.embedding.weight.data.copy_(vocab.vectors) # Loading pre-trained glove
    embeddings
    self.lstm = nn.LSTM(emb_size, hidden_size, num_layers)
    def forward(self, x):
    embedding = self.embedding(x)
    _, (hidden, cell) = self.lstm(embedding)
    return hidden, cell

class Decoder(nn.Module):
  def __init__(self, input_size, emb_size, hidden_size, num_layers, output_size):
    super(Decoder, self).__init__()
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.embedding = vocab_to_embedding_converter
    self.lstm = nn.LSTM(emb_size, hidden_size, num_layers)
    self.fc = nn.Linear(hidden_size, output_size)
  def forward(self, x, hidden, cell):
    x = x.unsqueeze(0)
    embedding = self.embedding(x)
    outputs, (hidden, cell) = self.lstm(embedding, (hidden, cell))
    preds = self.fc(outputs)
    preds = preds.squeeze(0)
    return preds, hidden, cell

class Seq2Seq(nn.Module):
  def __init__(self, encoder, decoder):
    super(Seq2Seq, self).__init__()
    self.encoder = encoder
    self.decoder = decoder
  def forward(self, source, target):
    target_len = target.shape[0]
    batch_size = source.shape[1]
    target_vocab_size = len(ml.vocab)
    outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)
    hidden, cell = self.encoder(source)
    x = target[0]
    for t in range(1, target_len):
    output, hidden, cell = self.decoder(x, hidden, cell)
    outputs[t] = output
    pred = output.argmax(1)
    x = target[t]
    return outputs

encoder = Encoder(input_size_encoder, encoder_embedding_size, hidden_size,
num_layers).to(device)
decoder = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, num_layers,
output_size).to(device)
model = Seq2Seq(encoder, decoder).to(device)
# state_dict = torch.load("./saved_models/model_weights.pth")
# model.load_state_dict(state_dict)
print("model loaded !")
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
pad_idx = en.vocab.stoi["<pad>"]
criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)

print('Starting training...')
# Train
avg_train_losses = list()
avg_val_losses = list()
for epoch in range(epochs):
  count = 0
  avg_train_loss = 0
  model.train()
  for batch_idx, batch in enumerate(train_iterator):
    count += 1
    source = batch.en.to(device)
    trg = batch.ml.to(device)
    output = model(source, trg)
    output = output[1:].reshape(-1, output.shape[2])
    trg = trg[1:].reshape(-1)
    print(output.shape, trg.shape)
    optimizer.zero_grad()
    loss = criterion(output, trg)
    avg_train_loss += loss.item()
    # Back prop
    loss.backward()
    if (batch_idx%1000):
    print(f'Epoch: {epoch}, Batch Index : {batch_idx}/{len(train_iterator)}, Train
    Loss:{loss.item()}')
    torch.save(model.state_dict(), 'saved_models/model_weights.pth')
    # Clip to avoid exploding gradient issues
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
    optimizer.step()
    # Plot to Tensorboard
    writer.add_scalar("Training loss", loss, global_step=step1)
    step1 += 1

  avg_train_loss = avg_train_loss/count
  avg_train_losses.append(avg_train_loss)
  count = 0
  avg_val_loss = 0
  for batch_idx, batch in enumerate(valid_iterator):
    count += 1
    source = batch.en.to(device)
    trg = batch.ml.to(device)
    output = model(source, trg)
    output = output[1:].reshape(-1, output.shape[2])
    trg = trg[1:].reshape(-1)
    optimizer.zero_grad()
    loss = criterion(output, trg)
    avg_val_loss += loss.item()
    print(loss.item())
    # Plot to tensorboard
    writer.add_scalar("Validation loss", loss, global_step=step2)
    step2 += 1
  print(f'Epochs:{epoch+1}, Average Train Loss:{avg_train_loss}, Average Validation
  Loss:{avg_val_loss}')
print('Done training !')

# Bleu score
def translate_en_ml(model, sentence, en, ml, device, max_length=50):
  print(f'Translating the English sentence : {sentence}')
  # Load English Tokenizer
  spacy_en = spacy.load("en_core_web_sm")
  if type(sentence) == str:
    tokens = [token.text.lower() for token in spacy_en(sentence)]
  else:
    tokens = [token.lower() for token in sentence]
  print(f'Tokens Generated : {tokens}')
  print('')
  tokens.insert(0, en.init_token)
  tokens.append(en.eos_token)
  text_to_indices = [en.vocab.stoi[token] for token in tokens]
  # Convert to tensor
  sent_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)
  # Build encoder hidden cell state
  with torch.no_grad():
    hidden, cell = model.encoder(sent_tensor)
  outputs = [ml.vocab.stoi["<sos>"]]

  for _ in range(max_length):
    first_word = torch.LongTensor([outputs[-1]]).to(device)
    with torch.no_grad():
      output, hidden, cell = model.decoder(first_word, hidden, cell)
    estimated = output.argmax(1).item()
    outputs.append(estimated)
    if output.argmax(1).item() == ml.vocab.stoi["<eos>"]:
    break
  translated_sentence = [ml.vocab.itos[idx] for idx in outputs]
  print(translated_sentence[1:])
  translated_sentence = tokenizer.convert_tokens_to_string(translated_sentence)
  print(translated_sentence)
  return translated_sentence[1:]

# Bleu score
def bleu(data, model, en, ml, device):
  outputs = []
  targets = []
  for ex in data:
    src = vars(ex)["en"]
    trg = vars(ex)["ml"]
    preds = translate_en_ml(model, src, en, ml, device)
    preds = preds[:-1]
    targets.append([trg])
    outputs.append(preds)
  return bleu_score(outputs, targets)
sentence = 'It turned out to be good.'
translated = translate_en_ml(model, sentence, en, ml, device)
print(f'Translated sentence :{translated}')

# print('Computing bleu scores..')
# # Train
# score = bleu(train_data, model, en, ml, device)
# print(f"Bleu score for Train data: {score*100:.2f}")
# # Validation
# score = bleu(val_data, model, en, ml, device)
# print(f"Bleu score for Validation data: {score*100:.2f}")
# # Test
# score = bleu(test_data, model, en, ml, device)
# print(f"Bleu score for Test data: {score*100:.2f}")